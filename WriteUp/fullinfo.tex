\section{High Information}\label{sec:high-info}
In this section, we consider a scenario where the defender fully knows the vulnerabilities of every configuration that they deploy. Furthermore, we assume that they defender knows the number of attackers. At every time step, we observe what vulnerability was attacked, which attacker attacked it and the reward obtained. Since we do not know the probability distribution among the attackers, we assume the attackers are chosen adversarially. This setting corresponds to an older application where enough research has been done to determine the vulnerabilities of each configuration that is being deployed and past data about attacks has helped determine the different kinds of attackers who are trying to exploit vulnerabilities in this software.

In this case, it is easy to see that when the rewards, themselves, are adversarial, the best policy regret one can obtain for this problem is $\Omega (T)$ [cite Arora et al]. However, when the rewards $r(\psi, e, c)$ are constant for each attacker, vulnerability and configuration tuple, then we can model this as a repeated game against a {\em master} attacker who chooses both an attacker and a vulnerability. \textbf{Need to figure this out}.

