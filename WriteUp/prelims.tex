\section{Model and Preliminaries}\label{sec:model}
We consider a repeated game played with two players - a {\em defender} (denoted by $\Theta$) and $\tau$ {\em attackers} (denoted by $ \Psi = \{\Psi_1, \Psi_2, \dots, \Psi_\tau\}$). At each round (or timestep) of this game, one of these attackers tries to exploit the application set up by the defender. We assume there exists a probability distribution $\cal P$ over the set attackers which decides which attacker attacks at a given round. The defender has a set $\cal C = \{c_1, c_2, \dots c_n\}$ of $n$ {\em configurations} that it can deploy. Each configuration has a set of {\em vulnerabilities} $\cal V_c = \{e_1, e_2, \dots, e_{|\cal V_c|}\}$ that can be exploited. Since each vulnerability has an exploit associated with it, we use thease two terms interchangably. We define the set of all vulnerabilities by  $\cal V = \bigcup_{c \in C} \cal V_c$. This vulnerability set for each configuration may not be known before hand but we assume that no configuration is perfect i.e. every configuration has some vulnerability. 

At the $t$'th round, the defender chooses a configuration to deploy (denoted by $v_t \in \cal C$) and the attacker $f(t)$ (where $f$ is a function that maps a round to the attacker at that round) chooses a vulnerability to exploit (denoted by $a_t$); the attacker can also choose not to exploit any vulnerability during a turn. Furthermore, we assume that there is a cost incurred when switching between configurations which is known a priori and remains constant throughout. This is not an unrealistic assumption to make since the switching cost can be estimated during the testing phase before the deployment of the application. The cost incurred by switching from configuration $c$ to configuration $c'$ in the $t$'th round is denoted by $s(c, c') \in (0,1]$. The game is played for $T$ rounds. 

For each vulnerability $e \in N$, for each configuration $c \in \cal C$, for each attacker type $\psi \in \Psi$, we define a reward to the defender at the $t$'th round denoted by $r_t(\psi, e, c)$. The reward $r_t(\psi, e, c) \in [-1, 0)$, if an attacker successfully exploits a vulnerability that the defender's configuration has i.e. $e \in \cal V_c$ and is $0$ otherwise. Note that the different attacker types will result in the defender obtaining different utilities because of the varying attacker capabilities e.g., some attackers may not have the expertise to carry out certain attacks \citep{sailik2016webappmtd}. For a specific attacker, we assume these rewards are constant throughout.

We make no assumptions about the attacker strategy since the attacker may not be fully rational. We also do not make assumptions about the attacker rewards, since these values will be hard to observe and even harder to find out beforehand. We however assume that the attacker has access to information from the previous rounds and is capable of reconnaissance.

Our goal in this paper is to maximise the total utility the defender receives, i.e.
\begin{align*}
    \sum_{t=1}^T r_t(\Psi_{f(t)}, a_t, v_t) - s_t(v_{t-1}, v_t)
\end{align*}

\subsection{Regret}\label{subsec:regret}
When we have infinitely many attacker types who are adversarially chosen, it is easy to see that this problem is very similar to the adversarial multi-armed bandit problem. This similarity suggest that algorithms like FPL+GR[cite] or Exp3 [cite] will output strategies with good external regret guarantees. More specifically, the algorithm minimizes the difference between the total welfare it obtains and the welfare of the best fixed strategy in hindsight. However, since the algorithms themselves do not consider switching cost, to use these algorithms, we must include the switching cost in the loss function. This gives us the following external regret guarantee:
\begin{align*}
    \max_{v \in \cal C} & \bigg (\sum_{t=1}^T r_t(\Psi_{f(t)}, a_t, v) - s_t(v_{t-1}, v) \bigg ) \\
    & - \bigg ( \sum_{t=1}^T r_t(\Psi_{f(t)}, a_t, v_t) - s_t(v_{t-1}, v_t) \bigg ) 
    \le \cal O(\sqrt{T})
\end{align*}

However, as one can see and as [cite Arora] argue, this notion of regret does not have any meaning when there are non-zero switching costs. [cite Arora] consider an attacker strategy which depends on all the previous configurations of the defender and define an alternate notion of regret known as policy regret. This regret takes into consideration how the attacker would have behaved if you had played a pure strategy instead and then compares the performance of the bandit algorithm to the performance of the best pure strategy in hindsight. More specifically, let $a_t$ be the attacker strategy at time $t$ which takes as input, the actions $v_1, v_2, \dots, v_{t-1}$. Then, the policy regret of an algorithm would evaluate to

\begin{align*}
    \max_{v \in \cal C} & \bigg (\sum_{t=1}^T r_t(\Psi_{f(t)}, a_t(v, v, \dots, v), v) \bigg ) \\
    & - \bigg ( \sum_{t=1}^T r_t(\Psi_{f(t)}, a_t(v_1, v_2, \dots, v_{t-1}), v_t) - s_t(v_{t-1}, v_t) \bigg ) 
\end{align*}

[cite Arora] show that the lower bound for policy regret is $\Omega (T)$ in the presence of an adaptive adversary. 

However, in a moving target defense context, the baseline used in policy regret does not make much sense either since it is a stationary baseline. This allows the attacker to perform reconnaissance and learn the best possible strategy to inflict damage on the defender. So this baseline can have an arbitrarily bad performance. This means that even though, in realistic settings any pure strategy will perform poorly, in theory no algorithm can guarantee doing better than this. From this, we conclude that even though the bandit approach may result in good welfare, the regret of these algorithms can be arbitrarily bad or can have no meaning.




