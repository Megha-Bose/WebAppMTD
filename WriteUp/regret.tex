\section{Regret Lower Bounds}\label{sec:lower-bounds}
When we only have one attacker type and the rewards at each round are adversarial, it is easy to see that this problem is very similar to the multi-armed bandit problem. This similarity allows us to use algorithms like FPL+GR[cite] or Exp3 [cite] to obtain strategies with good regret guarantees. More specifically, the algorithm minimizes the difference between the total welfare and the welfare of the best fixed strategy in hindsight. However, since the algorithms themselves do not consider switching cost, to use these algorithms, we must include the switching cost in the loss function. This gives us the following regret guarantee:
\begin{align*}
    \max_{v \in \cal C} & \bigg (\sum_{t=1}^T r_t(\Psi_{f(t)}, a_t, v) - s_t(v_{t-1}, v) \bigg ) \\
    & - \bigg ( \sum_{t=1}^T r_t(\Psi_{f(t)}, a_t, v_t) - s_t(v_{t-1}, v_t) \bigg ) 
    \le \cal O(\sqrt{T})
\end{align*}

However, as one can see and as [cite] argue, this notion of regret does not have any meaning when there are switching costs. They consider an attacker strategy which depends on all the previous configurations of the defender and define an alternate notion of regret known as policy regret. This regret takes into consideration how the attacker would have behaved if you had played a pure strategy instead and then compares the performance of the bandit algorithm to the performance of the best pure strategy in hindsight. More specifically, let $a_t$ be the attacker strategy at time $t$ which takes as input, the actions $v_1, v_2, \dots, v_{t-1}$. Then, the policy regret of an algorithm would evaluate to

\begin{align*}
    \max_{v \in \cal C} & \bigg (\sum_{t=1}^T r_t(\Psi_{f(t)}, a_t(v, v, \dots, v), v) \bigg ) \\
    & - \bigg ( \sum_{t=1}^T r_t(\Psi_{f(t)}, a_t(v_1, v_2, \dots, v_{t-1}), v_t) - s_t(v_{t-1}, v_t) \bigg ) 
\end{align*}

[cite] show that the lower bound for policy regret is $\Omega (T)$ in the presence of an adaptive adversary. 

However, in a moving target defense context, the baseline used in policy regret does not make much sense either since it is a stationary baseline. This allows the attacker to perform reconnaissance and learn the best possible strategy to inflict damage on the defender. So this baseline can have an arbitrarily bad performance. So for online learning for moving target defense, we introduce a new notion called {\em deception regret}. This combines the best of both regret notions. The baseline it compares the welfare with is the performance obtained when the defender plays the best pure strategy in hindsight assuming that the attacker has been {\em deceived} by the earlier switches of the defender. The deception regret of an algorithm is:
\begin{align}
    \max_{v \in \cal C} & \bigg (\sum_{t=1}^T r_t(\Psi_{f(t)}, a_t(v_1, v_2, \dots, v_{t-1}), v) \bigg ) \\
    & - \bigg ( \sum_{t=1}^T r_t(\Psi_{f(t)}, a_t(v_1, v_2, \dots, v_{t-1}), v_t) - s_t(v_{t-1}, v_t) \bigg ) \label{eq:deception-regret}
\end{align}

Unfortunately, even when the rewards are constant, when there is only one attacker and this attacker is capable of exploiting some vulnerability in all the configurations, the deception regret is $\Omega (T)$.

\begin{proposition}\label{prop:regret-lower-bound}
When there is only one attacker, then the deception regret given by Equation \eqref{eq:deception-regret} is lower bounded by \\ $-\min_{e \in \cal V}\max_{v \in \cal C} r(\psi_1, e, v) \frac{T}{|\cal C|}$
\end{proposition}
\begin{proof}

\end{proof}

What Proposition \ref{prop:regret-lower-bound} implies is that it will be impossible to come up with an algorithm which achieves sub-linear regret. Note that any strategy has an $\cal O(T)$ regret since the maximum possible regret is $2T$. Therefore our main focus in this work is on creating scalable heuristics that have high welfare when used on real life data with realistic attacker strategies.