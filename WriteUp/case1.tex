\section{Case 1}\label{sec:case1}
In this section, we consider a situation where we can observe the vulnerability being exploited as well as the attacker type trying to exploit the vulnerability. While this is similar to the model presented in \cite{sengupta2020bsmg}, our model does not observe the utility that the attacker receives.
Note that even if we do not know the probability distribution across attacker types, it would be fairly easy to estimate it since we know which attacker type attacks at every round. To come up with a solution for this case, we use an extension of the multi armed bandits for our setting. This algorithm is described in Algorithm \ref{alg:fpl-mtd}. - This part is unclear. Is the MAB extension used to model the problem or is it solution to the problem ? Either way we should present the MAB and the extension for it and then present the algorithm. Is this new algorithm or is it an existing algorithm that we are presenting as solution here ?

\begin{algorithm}
\DontPrintSemicolon
$K_r(a_t, v_t, \Psi_{f(t)}) = M$\;
\For{\(k = 1, 2, \dots, M\)}{
    Follow the FPL step in the previous algorithm to produce $\tilde{v}$ as a simulation of $v_t$\;
    \uIf{$a_t \in \cal V_{\tilde{v}}$}{
        $K_r(a_t, v_t, \Psi_{f(t)}) = \min(K_r(a_t, v_t, \Psi_{f(t)}), k)$\;
    }
}
$K_r(a_t, v_t, \Psi_{f(t)}) = \frac{K_r(a_t, v_t, \Psi_{f(t)})}{\cal P_{\Psi_{f(t)}}}$\;
\caption{GR-Reward}
\label{alg:gr-reward}
\end{algorithm}

\begin{algorithm}
\DontPrintSemicolon
$K_s(v_{t-1}, v_t) = M$\;
\For{\(k = 1, 2, \dots, M\)}{
    Follow the FPL step in the previous algorithm to produce \(\tilde{v}\) as a simulation of \(v_t\)\;
    \uIf{$\tilde{v} = v_t$}{
        $K_s(v_{t-1}, v_t) = \min(K_s(v_{t-1}, v_t), k)$\;
    }
}
\caption{GR-Switch}
\label{alg:gr-switch}
\end{algorithm}

\begin{algorithm}
\DontPrintSemicolon
$\hat{r}_{e,\psi} = 0 \quad \forall e \in N, \psi \in \Psi$\;
$\hat{s}_{c, c'} = 0 \quad \forall c, c' \in C$\;
\For{$t = 1,2,\dots, T$}{
    Sample $flag \in \{0,1\}$ such that $flag = 0$ with prob $\gamma$\;
    \uIf{$flag==0$}{
        Let $v_t$ be a randomly sampled configuration\;
    }
    \Else{
        Draw $z_{e, \psi} \gets exp(\eta)$ independently for $e \in N$ and $\psi \in \Psi$\;
        Let $u_c = \sum_{\psi \in \Psi} \min_{e \in \cal V_c} \cal P_{\psi} (\hat{r}_{e, \psi} - z_{e, \psi}) \quad \forall c \in C$\;
        Let $v_t = \max_{c \in C} e^{-g(c)} u_c - \hat{s}_{v_{t-1}, c}$\;
    }
    Adversary of type $\Psi_{f(t)}$ plays $a_t$, you play $v_t$ and get a reward $r_t(a_t, v_t)$ and incur a switching cost $s(v_{t-1}, v_t)$\;
    Run GR-Reward to estimate $\frac{1}{p(a_t \in \cal V_{v_t} \land \Psi_{f(t)})}$ as $K_r(a_t, v_t \Psi_{f(t)})$\;
    
    Run GR-Switch to estimate $\frac{1}{p(v_t/ v_{t-1})}$ as $K_s(v_{t-1}, v_t)$\;
    
    Update $\hat{r}_{a_t,\Psi_{f(t)}} = \hat{r}_{a_t,\Psi_{f(t)}} + K_r(a_t, v_t, \Psi_{f(t)})r_t(a_t, \Psi_{f(t)})$\;
    
    Update $\hat{s}_{v_{t-1}, v_t} = \hat{s}_{v_{t}, v_{t-1}} = \hat{s}_{v_{t-1}, v_t} + K_s(v_{t-1}, v_t) s(v_{t-1}, v_t)$\;
}
\caption{FPL-MTD}
\label{alg:fpl-mtd}
\end{algorithm}

The FPL-MTD algorithm retains the essence of the FPL-UE algorithm \citep{long2016fplue} but it has a few modifications to adapt it to our setting.  

We maintain a reward estimate $\hat{r}_{e, \psi}$ for each vulnerability $e$ and for each attacker type $\psi$. We also maintain a switching cost estimate $\hat{s}_{c, c'}$ for each pair of configurations. 

We then use two steps to decide which configuration (denoted by $v_t$) to deploy. First we estimate the worst case reward of each configuration and denote it by $u_c$
\begin{align*}
    u_c = \sum_{\psi \in \Psi} \min_{e \in \cal V_c} \cal P_{\psi} (\hat{r}_{e, \psi} - z_{e, \psi}) 
\end{align*}
This worst case value is a weighted average of the worst case estimate of all the attacker types where the weights are the probabilities of occurrence of each type.

Secondly, we use these worst case rewards to compute a strategy that maximises the worst case reward subject to a possible switching cost. We also take into consideration possible reconnaissance and penalise repeating strategies accordingly. This gives us the following assignment
\begin{align*}
    v_t = \max_{c \in C} e^{-g(c)} u_c - \hat{s}_{v_{t-1}, c}
\end{align*}
Here $g(c)$ is the dampening factor which penalises repeating configurations
\begin{align*}
    g(c) = 
    \begin{cases}
        \alpha & \text{if } c = v_{t-1} \\
        0 & otherwise
    \end{cases}
\end{align*}

The next step is to update the estimates of the algorithm. We update the two estimates as follows
\begin{align*}
    &\hat{r}_{a_t, \Psi_{f(t)}} = \hat{r}_{a_t, \Psi_{f(t)}} + r_t(a_t, \Psi_{f(t)})\frac{\mathbb{I}\{a_t \in \cal V_{v_t} \land \Psi_{f(t)}\}}{p(a_t \in \cal V_{v_t} \land \Psi_{f(t)})} \\
    &\hat{s}_{v_{t-1}, v_t} = \hat{s}_{v_{t-1}, v_t} + s_t(v_{t-1}, v_t)\frac{\mathbb{I}\{v_t/v_{t-1}\}}{p(v_t/v_{t-1})}
\end{align*}
where $\mathbb{I}$ is the identity function and $p$ refers to the probability. The term $a_t \in \cal V_{v_t} \land \Psi_{f(t)}$ refers to the event where the attacker $\Psi_{f(t)}$ exploits a vulnerability of the chosen configuration and $v_t/v_{t-1}$ refers to the event where configuration $v_t$ is chosen immediately after configuration $v_{t-1}$. 

Note that the terms $r_t(a_t, \Psi_{f(t)})\frac{\mathbb{I}\{a_t \in \cal V_{v_t} \land \Psi_{f(t)}\}}{p(a_t \in \cal V_{v_t} \land \Psi_{f(t)})}$ and \\ $s_t(v_{t-1}, v_t)\frac{\mathbb{I}\{v_t/v_{t-1}\}}{p(v_t/v_{t-1})}$ are unbiased estimators of $r_t(a_t, \Psi_{f(t)})$ and $s_t(v_{t-1}, v_t)$ respectively. This method is used extensively in online learning. Like prior online learning literature (see \cite{neuandbartok2013fpl}), we use geometric resampling to compute an estimate of the inverse of the above probabilities since they cannot be computed efficiently. 

The intuition for geometric resampling is as follows: the number of trials required for $v_t$ to be the chosen configuration given $v_{t-1}$ is a geometric distribution with mean $\frac{1}{p(v_t/v_{t-1})}$. So we simulate the configuration process till we choose $v_t$ and the number of trials would be an unbiased estimate of $\frac{1}{p(v_t/v_{t-1})}$. This can be seen in Algorithm \ref{alg:gr-switch}.

Similar logic can be applied to the reward setting as well. However, the problem with the reward setting as that we cannot simulate multiple attacker types since we do not the rewards but we can advantage of the fact that attacker type selection is an independent occurrence and we know the probabilities of an attacker type occurring i.e.
\begin{align*}
    p(a_t \in \cal V_{v_t} \land \Psi_{f(t)}) = p(a_t \in \cal V_{v_t}) p(\Psi_{f(t)}) 
\end{align*}
We know $p(\Psi_{f(t)})$ and we use geometric resampling to estimate $p(a_t \in \cal V_{v_t})$. This is described in Algorithm \ref{alg:gr-reward}.